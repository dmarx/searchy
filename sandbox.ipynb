{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imghdr # builtin\n",
    "from pathlib import Path\n",
    "import sqlite3\n",
    "\n",
    "import faiss\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "#from torchvision.datasets import ImageFolder\n",
    "from torchvision.io import read_image\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision import transforms\n",
    "\n",
    "# https://stackoverflow.com/questions/42462431/oserror-broken-data-stream-when-reading-image-file/47958486\n",
    "from PIL import Image, ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# push the default CLIP transforms upstream to the data loader, \n",
    "# should permit batching iamges since we'll have a resize operation\n",
    "# https://huggingface.co/transformers/model_doc/clip.html#transformers.CLIPFeatureExtractor\n",
    "clip_transforms = transforms.Compose([\n",
    "    #transforms.ToPILImage(),\n",
    "    transforms.Resize(size=224),\n",
    "    transforms.CenterCrop(224),\n",
    "    #transforms.ToPILImage(),\n",
    "    transforms.ToTensor(),  # PIL -> ToTensor :: [0,255] -> [0.,1.]\n",
    "    transforms.Normalize([0.48145466, 0.4578275, 0.40821073], \n",
    "                         [0.26862954, 0.26130258, 0.27577711]),\n",
    "    \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIP(torch.nn.Module):\n",
    "    n_feats = 5125\n",
    "    def __init__(self, model_string=\"openai/clip-vit-base-patch32\"):\n",
    "        super().__init__()\n",
    "        self._model_string = model_string\n",
    "        self.model = CLIPModel.from_pretrained(model_string)\n",
    "        self.processor = CLIPProcessor.from_pretrained(model_string)\n",
    "        self.model.eval()\n",
    "    def project_images(self, images, normalize=True, preprocessed=False):\n",
    "        if not preprocessed:\n",
    "            imgs = self.processor(images=images, return_tensors=\"pt\", padding=True)\n",
    "        else:\n",
    "            imgs = {'pixel_values':images}\n",
    "        feats = self.model.get_image_features(**imgs)\n",
    "        if normalize:\n",
    "            feats = self.normalize(feats)\n",
    "        return feats\n",
    "    def project_texts(self, texts, normalize=True):\n",
    "        txts = self.processor(text=texts, return_tensors=\"pt\", padding=True)\n",
    "        feats = self.model.get_text_features(**txts)\n",
    "        if normalize:\n",
    "            feats = self.normalize(feats)\n",
    "        return feats\n",
    "    def normalize(self, x):\n",
    "        return x / x.norm(dim=-1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# https://pytorch.org/tutorials/beginner/basics/data_tutorial.html#creating-a-custom-dataset-for-your-files\n",
    "# NB: might be able to use lightning datamodule to parallelize data processing\n",
    "class RecusiveImagesPath(Dataset):\n",
    "    def __init__(self, \n",
    "                 root='sample_data/images', \n",
    "                 transforms=clip_transforms,\n",
    "                 #assign_ids=True, # doesn't need to be optional...\n",
    "                 next_id=0\n",
    "                ):\n",
    "        if isinstance(root, str):\n",
    "            root = Path(root)\n",
    "        assert root.is_dir()\n",
    "        self.root = root\n",
    "        self.transforms = transforms\n",
    "        #self.assign_ids = assign_ids\n",
    "        self.next_id = next_id\n",
    "        self.get_image_paths()\n",
    "    def get_image_paths(self):\n",
    "        self.img_paths = []\n",
    "        for path_obj in self.root.glob('*'):\n",
    "            if self._already_indexed(path_obj):\n",
    "                continue\n",
    "            if imghdr.what(path_obj) is not None:\n",
    "                #self.img_paths.append(path_obj)\n",
    "                self.img_paths.append((path_obj, self.next_id))\n",
    "                self.next_id += 1\n",
    "    def _already_indexed(self, path_obj):\n",
    "        # implement this later, assume all images are new for now\n",
    "        return False\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "    def __getitem__(self, idx):\n",
    "        #path = str(self.img_paths[idx])\n",
    "        path_obj, im_id = self.img_paths[idx]\n",
    "        path = str(path_obj)\n",
    "        im = Image.open(path)\n",
    "        if self.transforms is not None:\n",
    "            im = self.transforms(im)\n",
    "        #return im, path\n",
    "        return im, path, im_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip = CLIP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_fname = 'sample_index.faissindex'\n",
    "index=faiss.read_index(index_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.ntotal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([8, 512]), True)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images = [im for im,p in dataset]\n",
    "im_embed = clip.project_images(images)\n",
    "im_embed.shape, im_embed.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 512]),\n",
       " tensor([1.0000, 1.0000, 1.0000, 1.0000], grad_fn=<CopyBackwards>))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = [\"my baloney has a first name\", \"tropical beach\", \"beach\", \"sand beach with palm trees on a nice day\"]\n",
    "#txts = clip.processor(text=texts, return_tensors=\"pt\", padding=True)\n",
    "txt_embed = clip.project_texts(texts)\n",
    "txt_embed.shape, txt_embed.norm(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.1683, 0.1853, 0.1770, 0.1981, 0.2090, 0.1492, 0.1972, 0.1972],\n",
       "         [0.2623, 0.2536, 0.1461, 0.1679, 0.1688, 0.1912, 0.1985, 0.1701],\n",
       "         [0.2609, 0.2563, 0.1553, 0.1691, 0.1775, 0.1463, 0.1899, 0.1892],\n",
       "         [0.2747, 0.2699, 0.1176, 0.1454, 0.1415, 0.1525, 0.1747, 0.1144]],\n",
       "        grad_fn=<MmBackward>),\n",
       " tensor([[16.8332, 18.5312, 17.7043, 19.8129, 20.9047, 14.9190, 19.7199, 19.7210],\n",
       "         [26.2350, 25.3636, 14.6130, 16.7930, 16.8836, 19.1164, 19.8490, 17.0067],\n",
       "         [26.0949, 25.6321, 15.5259, 16.9058, 17.7491, 14.6251, 18.9940, 18.9170],\n",
       "         [27.4703, 26.9878, 11.7571, 14.5414, 14.1530, 15.2501, 17.4696, 11.4387]],\n",
       "        grad_fn=<MulBackward0>))"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sims = txt_embed @ im_embed.T\n",
    "#torch.matmul(text_embeds, image_embeds.t()) * logit_scale\n",
    "logit_scale = clip.model.logit_scale.exp()\n",
    "sims, sims * logit_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(100.0000, grad_fn=<ExpBackward>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clip.model.logit_scale.exp() # uh.. ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 512])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#F.softmax(sims, dim=0)\n",
    "im_embed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "       grad_fn=<CopyBackwards>)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "im_embed.norm(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/facebookresearch/faiss/wiki/Getting-started\n",
    "n_feats = 512 # = im_embed.shape\n",
    "index = faiss.IndexFlatIP(n_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = im_embed.detach()\n",
    "index.add(x.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, 8)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.is_trained, index.ntotal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.20904651, 0.19812861, 0.19721013],\n",
       "        [0.26234967, 0.253636  , 0.1984902 ],\n",
       "        [0.2609494 , 0.25632125, 0.1899397 ],\n",
       "        [0.27470297, 0.26987785, 0.17469627]], dtype=float32),\n",
       " array([[4, 3, 7],\n",
       "        [0, 1, 6],\n",
       "        [0, 1, 6],\n",
       "        [0, 1, 6]], dtype=int64))"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = 3\n",
    "x = txt_embed.detach().numpy()\n",
    "D, I = index.search(x, k)\n",
    "D,I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "faiss.write_index(index, 'sample_index.faissindex')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 sample_data\\images\\beach1.jpg\n",
      "1 sample_data\\images\\beach2.jpg\n",
      "2 sample_data\\images\\book1.jpg\n",
      "3 sample_data\\images\\dog1.png\n",
      "4 sample_data\\images\\dog2.png\n",
      "5 sample_data\\images\\landscape1.jpg\n",
      "6 sample_data\\images\\portrait1.jpg\n",
      "7 sample_data\\images\\portrait2.jpg\n"
     ]
    }
   ],
   "source": [
    "[\n",
    "    \"my baloney has a first name\", \n",
    "    \"tropical beach\", \n",
    "    \"beach\", \n",
    "    \"sand beach with palm trees on a nice day\"]\n",
    "for i, p in enumerate(dataset.img_paths):\n",
    "    print(i,p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 424 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Let's try loading a bigger dataset\n",
    "# This folder has about 5k images\n",
    "fpath = r\"C:\\Users\\shagg\\Pictures\\phone backup\"\n",
    "dataset = RecusiveImagesPath(root=fpath) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 87.55937790870667)\n",
      "(1, 103.40788507461548)\n",
      "(1, 111.80389714241028)\n",
      "(1, 124.06582236289978)\n",
      "(1, 118.63201856613159)\n",
      "(2, 95.43580174446106)\n",
      "(2, 99.1537573337555)\n",
      "(2, 107.63567590713501)\n",
      "(2, 115.75678420066833)\n",
      "(2, 115.40816307067871)\n",
      "(4, 109.15526986122131)\n",
      "(4, 109.3173942565918)\n",
      "(4, 108.98661804199219)\n",
      "(4, 108.0500762462616)\n",
      "(4, 108.37364888191223)\n",
      "(8, 106.75039219856262)\n",
      "(8, 108.72889018058777)\n",
      "(8, 108.89992237091064)\n",
      "(8, 108.77169871330261)\n",
      "(8, 108.5592269897461)\n",
      "(16, 106.71030879020691)\n",
      "(16, 106.37933802604675)\n",
      "(16, 106.47861409187317)\n",
      "(16, 107.10803556442261)\n",
      "(16, 107.1734721660614)\n",
      "(32, 107.8037486076355)\n",
      "(32, 105.18110370635986)\n",
      "(32, 105.78199791908264)\n",
      "(32, 104.88599348068237)\n",
      "(32, 104.81387233734131)\n",
      "(64, 104.15468192100525)\n",
      "(64, 103.25387740135193)\n",
      "(64, 103.43574666976929)\n",
      "(64, 102.77426767349243)\n",
      "(64, 101.6763014793396)\n",
      "(128, 101.80644845962524)\n",
      "(128, 101.79952454566956)\n",
      "(128, 102.71944904327393)\n",
      "(128, 101.44365501403809)\n",
      "(128, 95.90809035301208)\n",
      "(256, 87.34305500984192)\n",
      "(256, 88.98091220855713)\n",
      "(256, 99.50674676895142)\n",
      "(256, 96.46851015090942)\n",
      "(256, 87.52513647079468)\n",
      "(512, 96.5098397731781)\n",
      "(512, 100.67550039291382)\n",
      "(512, 88.8571720123291)\n",
      "(512, 92.23456859588623)\n",
      "(512, 101.21820855140686)\n"
     ]
    }
   ],
   "source": [
    "#%%time\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "#from loguru import logger\n",
    "\n",
    "batch_sizes = 2**np.arange(5, 10)\n",
    "elapsed = [] # Iteration only\n",
    "elapsed_w_processing = []\n",
    "\n",
    "\n",
    "def try_batch_size(b, max_samples=1024):\n",
    "    n_feats = 512\n",
    "    #index = faiss.IndexFlatIP(n_feats)\n",
    "    img_loader = DataLoader(dataset, batch_size=int(b), shuffle=False)\n",
    "    k = (max_samples / b) -1\n",
    "    for i, (batch, paths, ids) in enumerate(img_loader):\n",
    "        im_embed = clip.project_images(batch, normalize=True, preprocessed=True)\n",
    "        #index.add(im_imbed)\n",
    "        if i == k:\n",
    "            break\n",
    "\n",
    "experiments = []\n",
    "n_tries = 5\n",
    "with torch.no_grad():\n",
    "    for b in batch_sizes:\n",
    "        tries = []\n",
    "        for _ in range(n_tries):\n",
    "            st = time.time()\n",
    "            ###############\n",
    "            try_batch_size(b)\n",
    "            #################\n",
    "            et = time.time()        \n",
    "            e = et - st\n",
    "            print((b, e))\n",
    "            experiments.append((b,e))\n",
    "            #elapsed.append(e)\n",
    "        #elapsed_w_processing.append(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments = [\n",
    "(1, 87.55937790870667),\n",
    "(1, 103.40788507461548),\n",
    "(1, 111.80389714241028),\n",
    "(1, 124.06582236289978),\n",
    "(1, 118.63201856613159),\n",
    "(2, 95.43580174446106),\n",
    "(2, 99.1537573337555),\n",
    "(2, 107.63567590713501),\n",
    "(2, 115.75678420066833),\n",
    "(2, 115.40816307067871),\n",
    "(4, 109.15526986122131),\n",
    "(4, 109.3173942565918),\n",
    "(4, 108.98661804199219),\n",
    "(4, 108.0500762462616),\n",
    "(4, 108.37364888191223),\n",
    "(8, 106.75039219856262),\n",
    "(8, 108.72889018058777),\n",
    "(8, 108.89992237091064),\n",
    "(8, 108.77169871330261),\n",
    "(8, 108.5592269897461),\n",
    "(16, 106.71030879020691),\n",
    "(16, 106.37933802604675),\n",
    "(16, 106.47861409187317),\n",
    "(16, 107.10803556442261),\n",
    "(16, 107.1734721660614),\n",
    "(32, 107.8037486076355),\n",
    "(32, 105.18110370635986),\n",
    "(32, 105.78199791908264),\n",
    "(32, 104.88599348068237),\n",
    "(32, 104.81387233734131),\n",
    "(64, 104.15468192100525),\n",
    "(64, 103.25387740135193),\n",
    "(64, 103.43574666976929),\n",
    "(64, 102.77426767349243),\n",
    "(64, 101.6763014793396),\n",
    "(128, 101.80644845962524),\n",
    "(128, 101.79952454566956),\n",
    "(128, 102.71944904327393),\n",
    "(128, 101.44365501403809),\n",
    "(128, 95.90809035301208),\n",
    "(256, 87.34305500984192),\n",
    "(256, 88.98091220855713),\n",
    "(256, 99.50674676895142),\n",
    "(256, 96.46851015090942),\n",
    "(256, 87.52513647079468),\n",
    "(512, 96.5098397731781),\n",
    "(512, 100.67550039291382),\n",
    "(512, 88.8571720123291),\n",
    "(512, 92.23456859588623),\n",
    "(512, 101.21820855140686),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='x', ylabel='y'>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "x,y = zip(*experiments)\n",
    "\n",
    "plt.scatter(x,y)\n",
    "plt.show()\n",
    "\n",
    "sns.boxplot(x='x',y='y', data=pd.DataFrame({'x':x,'y':y}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 181.6195933818817)\n",
      "(64, 182.2417778968811)\n",
      "(128, 178.59700441360474)\n",
      "(256, 177.95661282539368)\n",
      "(512, 189.41789293289185)\n"
     ]
    }
   ],
   "source": [
    "#%%time\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "from loguru import logger\n",
    "import gc\n",
    "\n",
    "batch_sizes = 2**np.arange(5, 10)\n",
    "elapsed = [] # Iteration only\n",
    "elapsed_w_processing = []\n",
    "\n",
    "\n",
    "def try_batch_size(b, max_samples=1024):\n",
    "    n_feats = 512\n",
    "    index = faiss.IndexFlatIP(n_feats)\n",
    "    img_loader = DataLoader(dataset, batch_size=int(b), shuffle=False)\n",
    "    k = (max_samples / b) -1\n",
    "    for i, (batch, paths, ids) in enumerate(img_loader):\n",
    "        im_embed = clip.project_images(batch, normalize=True, preprocessed=True)\n",
    "        index.add(im_embed.numpy())\n",
    "        if i == k:\n",
    "            break\n",
    "\n",
    "experiments = []\n",
    "n_tries = 1\n",
    "with torch.no_grad():\n",
    "    for b in batch_sizes:\n",
    "        tries = []\n",
    "        for _ in range(n_tries):\n",
    "            gc.collect()\n",
    "            st = time.time()\n",
    "            ###############\n",
    "            try_batch_size(b)\n",
    "            #################\n",
    "            et = time.time()        \n",
    "            e = et - st\n",
    "            print((b, e))\n",
    "            experiments.append((b,e))\n",
    "            #elapsed.append(e)\n",
    "        #elapsed_w_processing.append(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be44e1a63ee7484d8b508b8f719a09dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/337 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.010009527206420898\n"
     ]
    }
   ],
   "source": [
    "#from loguru import logger\n",
    "\n",
    "#from tqdm import tqdm\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "#index_fname = 'sample_index.faissindex'\n",
    "#index=faiss.read_index(index_fname)\n",
    "n_feats = 512\n",
    "index = faiss.IndexFlatIP(n_feats)\n",
    "index = faiss.IndexIDMap(index)\n",
    "\n",
    "#b=128\n",
    "b=16 # just so we dno't have to wait forever for first case to fail if it's gonna\n",
    "img_loader = DataLoader(dataset, batch_size=int(b), shuffle=False)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, (batch, paths, ids) in enumerate(tqdm(img_loader)):\n",
    "        im_embed = clip.project_images(batch, normalize=True, preprocessed=True)\n",
    "        #index.add(im_embed.numpy())\n",
    "        vecs, ids = im_embed.numpy(), np.array(ids)\n",
    "        index.add_with_ids(vecs, ids)\n",
    "\n",
    "# 17min for 5k images, vectors w/o ids\n",
    "        \n",
    "st = time.time()\n",
    "#faiss.write_index(index, 'test_5k.faissindex') # 0.0205\n",
    "faiss.write_index(index, 'test_5k_w_ids.faissindex')\n",
    "et = time.time()\n",
    "print(et-st)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([5376, 5377, 5378, 5379], dtype=int64), 5380)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids, index.ntotal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__swig_destroy__',\n",
       " '__weakref__',\n",
       " 'add',\n",
       " 'add_c',\n",
       " 'add_with_ids',\n",
       " 'add_with_ids_c',\n",
       " 'assign',\n",
       " 'assign_c',\n",
       " 'compute_residual',\n",
       " 'compute_residual_n',\n",
       " 'd',\n",
       " 'get_distance_computer',\n",
       " 'id_map',\n",
       " 'index',\n",
       " 'is_trained',\n",
       " 'metric_arg',\n",
       " 'metric_type',\n",
       " 'ntotal',\n",
       " 'own_fields',\n",
       " 'range_search',\n",
       " 'range_search_c',\n",
       " 'reconstruct',\n",
       " 'reconstruct_c',\n",
       " 'reconstruct_n',\n",
       " 'reconstruct_n_c',\n",
       " 'referenced_objects',\n",
       " 'remove_ids',\n",
       " 'remove_ids_c',\n",
       " 'reset',\n",
       " 'sa_code_size',\n",
       " 'sa_decode',\n",
       " 'sa_decode_c',\n",
       " 'sa_encode',\n",
       " 'sa_encode_c',\n",
       " 'search',\n",
       " 'search_and_reconstruct',\n",
       " 'search_and_reconstruct_c',\n",
       " 'search_c',\n",
       " 'this',\n",
       " 'thisown',\n",
       " 'train',\n",
       " 'train_c',\n",
       " 'verbose']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.020595788955688477\n"
     ]
    }
   ],
   "source": [
    "st = time.time()\n",
    "faiss.write_index(index, 'test_5k.faissindex')\n",
    "et = time.time()\n",
    "print(et-st)\n",
    "# no time at all!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5380, True)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.ntotal, index.is_trained # nice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "139"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 8min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "\n",
    "# How long does it take to iterate through this? How does batch size impact this?\n",
    "# What is the limit on batch size for a forward pass through the model?\n",
    "# Can I calibrate this somehow? What about recording paths of images that we've processed already?\n",
    "\n",
    "fpath = r\"C:\\Users\\shagg\\Pictures\\phone backup\"\n",
    "dataset = RecusiveImagesPath(root=fpath)\n",
    "\n",
    "#img_loader = DataLoader(dataset, batch_size=64, shuffle=False) # 9min 2s\n",
    "#img_loader = DataLoader(dataset, batch_size=1, shuffle=False) # 12min43s w/o transforms\n",
    "#img_loader = DataLoader(dataset, batch_size=2, shuffle=False)\n",
    "#img_loader = DataLoader(dataset, batch_size=4, shuffle=False) # 8m w transforms (necessary for elevated batch size)\n",
    "#img_loader = DataLoader(dataset, batch_size=8, shuffle=False) # 8min 28s\n",
    "img_loader = DataLoader(dataset, batch_size=16, shuffle=False) # 8min 32s\n",
    "# trying again with larger batch after adding transforms\n",
    "\n",
    "for i, _ in enumerate(img_loader):\n",
    "    continue\n",
    "# yiiikes... 12min43s just to iterate through the files and load them into tensors... yeesh.\n",
    "# seriously doubt I'm multiprocessing here. Possibly low batch size kills multiproc as option?\n",
    "\n",
    "\n",
    "# TypeError: default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; \n",
    "# found <class 'PIL.JpegImagePlugin.JpegImageFile'>\n",
    "#\n",
    "# fixed above error with torchvision.io.read_image\n",
    "# Now we've got an uncollated shape issue\n",
    "#   RuntimeError: stack expects each tensor to be equal size, but got [3, 3024, 4032] at entry 0 and [3, 2268, 4032] at entry 16\n",
    "# could add a collate fn per: https://discuss.pytorch.org/t/dataloader-gives-stack-expects-each-tensor-to-be-equal-size-due-to-different-image-has-different-objects-number/91941\n",
    "# ... but then the padding would probably fuck up the CLIP featurization\n",
    "# could go the other way with a crop, but who even knows what I'd be cropping. How even does the model address the input resolution?\n",
    "# whatever, let's see how long it takes with a batch_size of 1\n",
    "#\n",
    "# RuntimeError: Unsupported marker type 0xa3\n",
    "# 'C:\\\\Users\\\\shagg\\\\Pictures\\\\phone backup\\\\20180913_145729.jpg' - panoramic, 35mb. \n",
    "#\n",
    "# crushing error by returning tensor of zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's see what happens if we turn up the num_workers\n",
    "fpath = r\"C:\\Users\\shagg\\Pictures\\phone backup\"\n",
    "dataset = RecusiveImagesPath(root=fpath)\n",
    "\n",
    "#img_loader = DataLoader(dataset, batch_size=16, shuffle=False) # 8min 32s\n",
    "#img_loader = DataLoader(dataset, batch_size=16, shuffle=False, num_workers=4) # \n",
    "# yeeeaaah.... this slow af for some reason.\n",
    "#for i, _ in enumerate(img_loader):\n",
    "#    continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class DataLoader in module torch.utils.data.dataloader:\n",
      "\n",
      "class DataLoader(typing.Generic)\n",
      " |  DataLoader(*args, **kwds)\n",
      " |  \n",
      " |  Data loader. Combines a dataset and a sampler, and provides an iterable over\n",
      " |  the given dataset.\n",
      " |  \n",
      " |  The :class:`~torch.utils.data.DataLoader` supports both map-style and\n",
      " |  iterable-style datasets with single- or multi-process loading, customizing\n",
      " |  loading order and optional automatic batching (collation) and memory pinning.\n",
      " |  \n",
      " |  See :py:mod:`torch.utils.data` documentation page for more details.\n",
      " |  \n",
      " |  Args:\n",
      " |      dataset (Dataset): dataset from which to load the data.\n",
      " |      batch_size (int, optional): how many samples per batch to load\n",
      " |          (default: ``1``).\n",
      " |      shuffle (bool, optional): set to ``True`` to have the data reshuffled\n",
      " |          at every epoch (default: ``False``).\n",
      " |      sampler (Sampler or Iterable, optional): defines the strategy to draw\n",
      " |          samples from the dataset. Can be any ``Iterable`` with ``__len__``\n",
      " |          implemented. If specified, :attr:`shuffle` must not be specified.\n",
      " |      batch_sampler (Sampler or Iterable, optional): like :attr:`sampler`, but\n",
      " |          returns a batch of indices at a time. Mutually exclusive with\n",
      " |          :attr:`batch_size`, :attr:`shuffle`, :attr:`sampler`,\n",
      " |          and :attr:`drop_last`.\n",
      " |      num_workers (int, optional): how many subprocesses to use for data\n",
      " |          loading. ``0`` means that the data will be loaded in the main process.\n",
      " |          (default: ``0``)\n",
      " |      collate_fn (callable, optional): merges a list of samples to form a\n",
      " |          mini-batch of Tensor(s).  Used when using batched loading from a\n",
      " |          map-style dataset.\n",
      " |      pin_memory (bool, optional): If ``True``, the data loader will copy Tensors\n",
      " |          into CUDA pinned memory before returning them.  If your data elements\n",
      " |          are a custom type, or your :attr:`collate_fn` returns a batch that is a custom type,\n",
      " |          see the example below.\n",
      " |      drop_last (bool, optional): set to ``True`` to drop the last incomplete batch,\n",
      " |          if the dataset size is not divisible by the batch size. If ``False`` and\n",
      " |          the size of dataset is not divisible by the batch size, then the last batch\n",
      " |          will be smaller. (default: ``False``)\n",
      " |      timeout (numeric, optional): if positive, the timeout value for collecting a batch\n",
      " |          from workers. Should always be non-negative. (default: ``0``)\n",
      " |      worker_init_fn (callable, optional): If not ``None``, this will be called on each\n",
      " |          worker subprocess with the worker id (an int in ``[0, num_workers - 1]``) as\n",
      " |          input, after seeding and before data loading. (default: ``None``)\n",
      " |      generator (torch.Generator, optional): If not ``None``, this RNG will be used\n",
      " |          by RandomSampler to generate random indexes and multiprocessing to generate\n",
      " |          `base_seed` for workers. (default: ``None``)\n",
      " |      prefetch_factor (int, optional, keyword-only arg): Number of samples loaded\n",
      " |          in advance by each worker. ``2`` means there will be a total of\n",
      " |          2 * num_workers samples prefetched across all workers. (default: ``2``)\n",
      " |      persistent_workers (bool, optional): If ``True``, the data loader will not shutdown\n",
      " |          the worker processes after a dataset has been consumed once. This allows to\n",
      " |          maintain the workers `Dataset` instances alive. (default: ``False``)\n",
      " |  \n",
      " |  \n",
      " |  .. warning:: If the ``spawn`` start method is used, :attr:`worker_init_fn`\n",
      " |               cannot be an unpicklable object, e.g., a lambda function. See\n",
      " |               :ref:`multiprocessing-best-practices` on more details related\n",
      " |               to multiprocessing in PyTorch.\n",
      " |  \n",
      " |  .. warning:: ``len(dataloader)`` heuristic is based on the length of the sampler used.\n",
      " |               When :attr:`dataset` is an :class:`~torch.utils.data.IterableDataset`,\n",
      " |               it instead returns an estimate based on ``len(dataset) / batch_size``, with proper\n",
      " |               rounding depending on :attr:`drop_last`, regardless of multi-process loading\n",
      " |               configurations. This represents the best guess PyTorch can make because PyTorch\n",
      " |               trusts user :attr:`dataset` code in correctly handling multi-process\n",
      " |               loading to avoid duplicate data.\n",
      " |  \n",
      " |               However, if sharding results in multiple workers having incomplete last batches,\n",
      " |               this estimate can still be inaccurate, because (1) an otherwise complete batch can\n",
      " |               be broken into multiple ones and (2) more than one batch worth of samples can be\n",
      " |               dropped when :attr:`drop_last` is set. Unfortunately, PyTorch can not detect such\n",
      " |               cases in general.\n",
      " |  \n",
      " |               See `Dataset Types`_ for more details on these two types of datasets and how\n",
      " |               :class:`~torch.utils.data.IterableDataset` interacts with\n",
      " |               `Multi-process data loading`_.\n",
      " |  \n",
      " |  .. warning:: See :ref:`reproducibility`, and :ref:`dataloader-workers-random-seed`, and\n",
      " |               :ref:`data-loading-randomness` notes for random seed related questions.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      DataLoader\n",
      " |      typing.Generic\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, dataset: torch.utils.data.dataset.Dataset[+T_co], batch_size: Union[int, NoneType] = 1, shuffle: bool = False, sampler: Union[torch.utils.data.sampler.Sampler[int], NoneType] = None, batch_sampler: Union[torch.utils.data.sampler.Sampler[Sequence[int]], NoneType] = None, num_workers: int = 0, collate_fn: Union[Callable[[List[~T]], Any], NoneType] = None, pin_memory: bool = False, drop_last: bool = False, timeout: float = 0, worker_init_fn: Union[Callable[[int], NoneType], NoneType] = None, multiprocessing_context=None, generator=None, *, prefetch_factor: int = 2, persistent_workers: bool = False)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  __iter__(self) -> '_BaseDataLoaderIter'\n",
      " |      # We quote '_BaseDataLoaderIter' since it isn't defined yet and the definition can't be moved up\n",
      " |      # since '_BaseDataLoaderIter' references 'DataLoader'.\n",
      " |  \n",
      " |  __len__(self) -> int\n",
      " |  \n",
      " |  __setattr__(self, attr, val)\n",
      " |      Implement setattr(self, name, value).\n",
      " |  \n",
      " |  check_worker_number_rationality(self)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  multiprocessing_context\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __annotations__ = {'_iterator': typing.Union[ForwardRef('_BaseDataLoad...\n",
      " |  \n",
      " |  __orig_bases__ = (typing.Generic[+T_co],)\n",
      " |  \n",
      " |  __parameters__ = (+T_co,)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from typing.Generic:\n",
      " |  \n",
      " |  __class_getitem__(params) from builtins.type\n",
      " |  \n",
      " |  __init_subclass__(*args, **kwargs) from builtins.type\n",
      " |      This method is called when a class is subclassed.\n",
      " |      \n",
      " |      The default implementation does nothing. It may be\n",
      " |      overridden to extend subclasses.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods inherited from typing.Generic:\n",
      " |  \n",
      " |  __new__(cls, *args, **kwds)\n",
      " |      Create and return a new object.  See help(type) for accurate signature.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(DataLoader) # num_workers , prefetch_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(46,\n",
       " ('C:\\\\Users\\\\shagg\\\\Pictures\\\\phone backup\\\\20180913_144244.jpg',\n",
       "  'C:\\\\Users\\\\shagg\\\\Pictures\\\\phone backup\\\\20180913_144258.jpg',\n",
       "  'C:\\\\Users\\\\shagg\\\\Pictures\\\\phone backup\\\\20180913_144303.jpg',\n",
       "  'C:\\\\Users\\\\shagg\\\\Pictures\\\\phone backup\\\\20180913_144648.jpg'))"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# OSError: broken data stream when reading image file\n",
    "i, _[1] # the fuck? Why is path 4 files? oh right, batched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(94,\n",
       " ('C:\\\\Users\\\\shagg\\\\Pictures\\\\phone backup\\\\20180913_145647.jpg',\n",
       "  'C:\\\\Users\\\\shagg\\\\Pictures\\\\phone backup\\\\20180913_145648.jpg'))"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# OSError: broken data stream when reading image file\n",
    "i, _[1] # \n",
    "# looks like the issue is with 'C:\\\\Users\\\\shagg\\\\Pictures\\\\phone backup\\\\20180913_145648.jpg' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.5255, 0.5216, 0.5255,  ..., 0.1961, 0.2078, 0.1333],\n",
       "         [0.5176, 0.5176, 0.5255,  ..., 0.2078, 0.1961, 0.1333],\n",
       "         [0.5216, 0.5255, 0.5333,  ..., 0.1686, 0.1529, 0.1294],\n",
       "         ...,\n",
       "         [0.9804, 0.9765, 0.9765,  ..., 0.0275, 0.0392, 0.0392],\n",
       "         [0.9765, 0.9765, 0.9765,  ..., 0.0235, 0.0353, 0.0314],\n",
       "         [0.9765, 0.9725, 0.9765,  ..., 0.0157, 0.0314, 0.0353]],\n",
       "\n",
       "        [[0.6196, 0.6157, 0.6118,  ..., 0.1569, 0.1686, 0.0941],\n",
       "         [0.6118, 0.6118, 0.6118,  ..., 0.1686, 0.1569, 0.0941],\n",
       "         [0.6078, 0.6118, 0.6196,  ..., 0.1294, 0.1137, 0.0902],\n",
       "         ...,\n",
       "         [0.9765, 0.9725, 0.9725,  ..., 0.0235, 0.0353, 0.0353],\n",
       "         [0.9725, 0.9725, 0.9725,  ..., 0.0196, 0.0314, 0.0275],\n",
       "         [0.9725, 0.9686, 0.9725,  ..., 0.0118, 0.0275, 0.0314]],\n",
       "\n",
       "        [[0.7608, 0.7569, 0.7569,  ..., 0.1608, 0.1725, 0.0980],\n",
       "         [0.7529, 0.7529, 0.7569,  ..., 0.1725, 0.1608, 0.0980],\n",
       "         [0.7529, 0.7569, 0.7647,  ..., 0.1333, 0.1176, 0.0941],\n",
       "         ...,\n",
       "         [0.9608, 0.9569, 0.9569,  ..., 0.0078, 0.0196, 0.0196],\n",
       "         [0.9569, 0.9569, 0.9569,  ..., 0.0039, 0.0157, 0.0118],\n",
       "         [0.9569, 0.9529, 0.9569,  ..., 0.0000, 0.0118, 0.0157]]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "im_test = Image.open('C:\\\\Users\\\\shagg\\\\Pictures\\\\phone backup\\\\20180913_145729.jpg')\n",
    "#im_test # PIL at least can read the last frame of the panorama.\n",
    "ToTensor()(im_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.5255, 0.5216, 0.5255,  ..., 0.1961, 0.2078, 0.1333],\n",
       "         [0.5176, 0.5176, 0.5255,  ..., 0.2078, 0.1961, 0.1333],\n",
       "         [0.5216, 0.5255, 0.5333,  ..., 0.1686, 0.1529, 0.1294],\n",
       "         ...,\n",
       "         [0.9804, 0.9765, 0.9765,  ..., 0.0275, 0.0392, 0.0392],\n",
       "         [0.9765, 0.9765, 0.9765,  ..., 0.0235, 0.0353, 0.0314],\n",
       "         [0.9765, 0.9725, 0.9765,  ..., 0.0157, 0.0314, 0.0353]],\n",
       "\n",
       "        [[0.6196, 0.6157, 0.6118,  ..., 0.1569, 0.1686, 0.0941],\n",
       "         [0.6118, 0.6118, 0.6118,  ..., 0.1686, 0.1569, 0.0941],\n",
       "         [0.6078, 0.6118, 0.6196,  ..., 0.1294, 0.1137, 0.0902],\n",
       "         ...,\n",
       "         [0.9765, 0.9725, 0.9725,  ..., 0.0235, 0.0353, 0.0353],\n",
       "         [0.9725, 0.9725, 0.9725,  ..., 0.0196, 0.0314, 0.0275],\n",
       "         [0.9725, 0.9686, 0.9725,  ..., 0.0118, 0.0275, 0.0314]],\n",
       "\n",
       "        [[0.7608, 0.7569, 0.7569,  ..., 0.1608, 0.1725, 0.0980],\n",
       "         [0.7529, 0.7529, 0.7569,  ..., 0.1725, 0.1608, 0.0980],\n",
       "         [0.7529, 0.7569, 0.7647,  ..., 0.1333, 0.1176, 0.0941],\n",
       "         ...,\n",
       "         [0.9608, 0.9569, 0.9569,  ..., 0.0078, 0.0196, 0.0196],\n",
       "         [0.9569, 0.9569, 0.9569,  ..., 0.0039, 0.0157, 0.0118],\n",
       "         [0.9569, 0.9529, 0.9569,  ..., 0.0000, 0.0118, 0.0157]]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ToTensor()(im_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
