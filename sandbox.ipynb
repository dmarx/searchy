{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imghdr # builtin\n",
    "from pathlib import Path\n",
    "import sqlite3\n",
    "\n",
    "import faiss\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "#from torchvision.datasets import ImageFolder\n",
    "from torchvision.io import read_image\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# push the default CLIP transforms upstream to the data loader, \n",
    "# should permit batching iamges since we'll have a resize operation\n",
    "# https://huggingface.co/transformers/model_doc/clip.html#transformers.CLIPFeatureExtractor\n",
    "clip_transforms = transforms.Compose([\n",
    "    #transforms.ToPILImage(),\n",
    "    transforms.Resize(size=224),\n",
    "    transforms.CenterCrop(224),\n",
    "    #transforms.ToPILImage(),\n",
    "    transforms.ToTensor(),  # PIL -> ToTensor :: [0,255] -> [0.,1.]\n",
    "    transforms.Normalize([0.48145466, 0.4578275, 0.40821073], \n",
    "                         [0.26862954, 0.26130258, 0.27577711]),\n",
    "    \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0113, -0.0988, -0.1134,  ...,  0.5873,  0.6749,  0.8209],\n",
       "         [-0.0259, -0.0405, -0.0696,  ...,  0.5143,  0.5727,  0.6165],\n",
       "         [-0.0259, -0.0259, -0.0988,  ...,  0.9084,  0.7187,  0.6311],\n",
       "         ...,\n",
       "         [ 0.3099, -0.0113,  0.1785,  ..., -0.0988, -0.1280,  0.0617],\n",
       "         [ 0.3975,  0.5289,  0.2077,  ..., -0.1280,  0.1493, -0.3470],\n",
       "         [-0.2302,  0.0179,  0.3683,  ...,  0.5435,  0.7041,  0.1931]],\n",
       "\n",
       "        [[ 0.6191,  0.5141,  0.5441,  ...,  0.8893,  1.0093,  1.1744],\n",
       "         [ 0.6041,  0.5891,  0.5891,  ...,  0.8292,  0.9043,  0.9343],\n",
       "         [ 0.6041,  0.6041,  0.5591,  ...,  1.2344,  1.0243,  0.9643],\n",
       "         ...,\n",
       "         [-0.2363, -0.5665, -0.3714,  ..., -0.6115, -0.6565, -0.4614],\n",
       "         [-0.1613, -0.0262, -0.3414,  ..., -0.6415, -0.3714, -0.8816],\n",
       "         [-0.7316, -0.5515, -0.2063,  ...,  0.0488,  0.2139, -0.3114]],\n",
       "\n",
       "        [[ 1.6482,  1.5771,  1.5913,  ...,  1.5202,  1.6055,  1.7620],\n",
       "         [ 1.6340,  1.6198,  1.6340,  ...,  1.4207,  1.5060,  1.5629],\n",
       "         [ 1.6340,  1.6340,  1.6055,  ...,  1.7762,  1.6340,  1.5629],\n",
       "         ...,\n",
       "         [-0.4990, -0.8119, -0.6270,  ..., -0.8972, -0.8972, -0.7123],\n",
       "         [-0.3853, -0.2573, -0.5701,  ..., -0.9114, -0.6270, -1.0821],\n",
       "         [-0.9541, -0.7550, -0.4279,  ..., -0.2715, -0.1151, -0.6128]]])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clip_transforms(im)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BICUBIC',\n",
       " 'BILINEAR',\n",
       " 'BOX',\n",
       " 'HAMMING',\n",
       " 'LANCZOS',\n",
       " 'NEAREST',\n",
       " '__class__',\n",
       " '__doc__',\n",
       " '__members__',\n",
       " '__module__']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torchvision\n",
    "dir(torchvision.transforms.InterpolationMode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIP(torch.nn.Module):\n",
    "    def __init__(self, model_string=\"openai/clip-vit-base-patch32\"):\n",
    "        super().__init__()\n",
    "        self._model_string = model_string\n",
    "        self.model = CLIPModel.from_pretrained(model_string)\n",
    "        self.processor = CLIPProcessor.from_pretrained(model_string)\n",
    "        self.model.eval()\n",
    "    def project_images(self, images, normalize=True):\n",
    "        imgs = self.processor(images=images, return_tensors=\"pt\", padding=True)\n",
    "        feats = self.model.get_image_features(**imgs)\n",
    "        if normalize:\n",
    "            feats = self.normalize(feats)\n",
    "        return feats\n",
    "    def project_texts(self, texts, normalize=True):\n",
    "        txts = self.processor(text=texts, return_tensors=\"pt\", padding=True)\n",
    "        feats = self.model.get_text_features(**txts)\n",
    "        if normalize:\n",
    "            feats = self.normalize(feats)\n",
    "        return feats\n",
    "    def normalize(self, x):\n",
    "        return x / x.norm(dim=-1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0259, -0.0696, -0.0988,  ...,  0.5873,  0.6749,  0.7771],\n",
       "         [-0.0259, -0.0550, -0.0842,  ...,  0.5727,  0.6165,  0.6457],\n",
       "         [-0.0550, -0.0405, -0.0696,  ...,  0.8209,  0.6895,  0.5873],\n",
       "         ...,\n",
       "         [ 0.3391,  0.3391,  0.3829,  ..., -0.0259,  0.0033, -0.0696],\n",
       "         [ 0.2661,  0.2953,  0.3245,  ...,  0.0763,  0.0617, -0.0405],\n",
       "         [-0.0550,  0.1493,  0.3537,  ...,  0.4121,  0.3391,  0.2077]],\n",
       "\n",
       "        [[ 0.6041,  0.5591,  0.5591,  ...,  0.9043,  1.0093,  1.1294],\n",
       "         [ 0.6041,  0.5891,  0.5741,  ...,  0.8893,  0.9493,  0.9793],\n",
       "         [ 0.5741,  0.5891,  0.5741,  ...,  1.1444,  1.0093,  0.9193],\n",
       "         ...,\n",
       "         [-0.2063, -0.2213, -0.1613,  ..., -0.5515, -0.5215, -0.5965],\n",
       "         [-0.2663, -0.2513, -0.2363,  ..., -0.4464, -0.4614, -0.5665],\n",
       "         [-0.5515, -0.3864, -0.2063,  ..., -0.0862, -0.1613, -0.2963]],\n",
       "\n",
       "        [[ 1.6340,  1.6055,  1.6055,  ...,  1.5344,  1.6055,  1.7193],\n",
       "         [ 1.6340,  1.6198,  1.6198,  ...,  1.4918,  1.5487,  1.5913],\n",
       "         [ 1.6055,  1.6198,  1.6198,  ...,  1.6909,  1.5913,  1.5202],\n",
       "         ...,\n",
       "         [-0.4706, -0.4706, -0.4279,  ..., -0.8119, -0.7692, -0.8261],\n",
       "         [-0.4990, -0.4848, -0.4706,  ..., -0.7266, -0.7266, -0.7977],\n",
       "         [-0.7692, -0.6128, -0.4422,  ..., -0.4137, -0.4706, -0.5844]]])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://pytorch.org/tutorials/beginner/basics/data_tutorial.html#creating-a-custom-dataset-for-your-files\n",
    "# NB: might be able to use lightning datamodule to parallelize data processing\n",
    "class RecusiveImagesPath(Dataset):\n",
    "    def __init__(self, root='sample_data/images', transforms=clip_transforms):\n",
    "        self.root = root\n",
    "        self.get_image_paths()\n",
    "        self.transforms = transforms\n",
    "    def get_image_paths(self):\n",
    "        self.img_paths = []\n",
    "        for path_obj in Path(self.root).glob('*'):\n",
    "            if imghdr.what(path_obj) is not None:\n",
    "                self.img_paths.append(path_obj)\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "    def __getitem__(self, idx):\n",
    "        path = str(self.img_paths[idx])\n",
    "        im = Image.open(path)\n",
    "        #return Image.open(path), path\n",
    "        #try:\n",
    "        #    im = read_image(path)\n",
    "        #except RuntimeError:\n",
    "        #    #tensorize = ToTensor()\n",
    "        #    #im_pil = Image.open(path)\n",
    "        #    #im = tensorize(im_pil)\n",
    "        #    im = torch.zeros(3,10,10)\n",
    "        if self.transforms is not None:\n",
    "            im = self.transforms(im)\n",
    "        return im, path\n",
    "        \n",
    "dataset = RecusiveImagesPath()\n",
    "[(im,p) for im,p in dataset]\n",
    "im = dataset[0][0]\n",
    "im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip = CLIP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_fname = 'sample_index.faissindex'\n",
    "index=faiss.read_index(index_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.ntotal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([8, 512]), True)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images = [im for im,p in dataset]\n",
    "im_embed = clip.project_images(images)\n",
    "im_embed.shape, im_embed.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 512]),\n",
       " tensor([1.0000, 1.0000, 1.0000, 1.0000], grad_fn=<CopyBackwards>))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = [\"my baloney has a first name\", \"tropical beach\", \"beach\", \"sand beach with palm trees on a nice day\"]\n",
    "#txts = clip.processor(text=texts, return_tensors=\"pt\", padding=True)\n",
    "txt_embed = clip.project_texts(texts)\n",
    "txt_embed.shape, txt_embed.norm(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.1683, 0.1853, 0.1770, 0.1981, 0.2090, 0.1492, 0.1972, 0.1972],\n",
       "         [0.2623, 0.2536, 0.1461, 0.1679, 0.1688, 0.1912, 0.1985, 0.1701],\n",
       "         [0.2609, 0.2563, 0.1553, 0.1691, 0.1775, 0.1463, 0.1899, 0.1892],\n",
       "         [0.2747, 0.2699, 0.1176, 0.1454, 0.1415, 0.1525, 0.1747, 0.1144]],\n",
       "        grad_fn=<MmBackward>),\n",
       " tensor([[16.8332, 18.5312, 17.7043, 19.8129, 20.9047, 14.9190, 19.7199, 19.7210],\n",
       "         [26.2350, 25.3636, 14.6130, 16.7930, 16.8836, 19.1164, 19.8490, 17.0067],\n",
       "         [26.0949, 25.6321, 15.5259, 16.9058, 17.7491, 14.6251, 18.9940, 18.9170],\n",
       "         [27.4703, 26.9878, 11.7571, 14.5414, 14.1530, 15.2501, 17.4696, 11.4387]],\n",
       "        grad_fn=<MulBackward0>))"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sims = txt_embed @ im_embed.T\n",
    "#torch.matmul(text_embeds, image_embeds.t()) * logit_scale\n",
    "logit_scale = clip.model.logit_scale.exp()\n",
    "sims, sims * logit_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(100.0000, grad_fn=<ExpBackward>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clip.model.logit_scale.exp() # uh.. ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 512])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#F.softmax(sims, dim=0)\n",
    "im_embed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "       grad_fn=<CopyBackwards>)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "im_embed.norm(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/facebookresearch/faiss/wiki/Getting-started\n",
    "n_feats = 512 # = im_embed.shape\n",
    "index = faiss.IndexFlatIP(n_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = im_embed.detach()\n",
    "index.add(x.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, 8)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.is_trained, index.ntotal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.20904651, 0.19812861, 0.19721013],\n",
       "        [0.26234967, 0.253636  , 0.1984902 ],\n",
       "        [0.2609494 , 0.25632125, 0.1899397 ],\n",
       "        [0.27470297, 0.26987785, 0.17469627]], dtype=float32),\n",
       " array([[4, 3, 7],\n",
       "        [0, 1, 6],\n",
       "        [0, 1, 6],\n",
       "        [0, 1, 6]], dtype=int64))"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = 3\n",
    "x = txt_embed.detach().numpy()\n",
    "D, I = index.search(x, k)\n",
    "D,I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "faiss.write_index(index, 'sample_index.faissindex')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 sample_data\\images\\beach1.jpg\n",
      "1 sample_data\\images\\beach2.jpg\n",
      "2 sample_data\\images\\book1.jpg\n",
      "3 sample_data\\images\\dog1.png\n",
      "4 sample_data\\images\\dog2.png\n",
      "5 sample_data\\images\\landscape1.jpg\n",
      "6 sample_data\\images\\portrait1.jpg\n",
      "7 sample_data\\images\\portrait2.jpg\n"
     ]
    }
   ],
   "source": [
    "[\n",
    "    \"my baloney has a first name\", \n",
    "    \"tropical beach\", \n",
    "    \"beach\", \n",
    "    \"sand beach with palm trees on a nice day\"]\n",
    "for i, p in enumerate(dataset.img_paths):\n",
    "    print(i,p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 369 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Let's try loading a bigger dataset\n",
    "# This folder has about 5k images\n",
    "fpath = r\"C:\\Users\\shagg\\Pictures\\phone backup\"\n",
    "dataset = RecusiveImagesPath(root=fpath) \n",
    "# takes a lot of time just to build this object...\n",
    "# I'm guessing the imghdr call slows things down a bunch\n",
    "# trivially fast on reload, so at least we're caching\n",
    "# Nope, faster after rebuilding the class too. weird."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 8min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# https://stackoverflow.com/questions/42462431/oserror-broken-data-stream-when-reading-image-file/47958486\n",
    "from PIL import Image, ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "# How long does it take to iterate through this? How does batch size impact this?\n",
    "# What is the limit on batch size for a forward pass through the model?\n",
    "# Can I calibrate this somehow? What about recording paths of images that we've processed already?\n",
    "\n",
    "fpath = r\"C:\\Users\\shagg\\Pictures\\phone backup\"\n",
    "dataset = RecusiveImagesPath(root=fpath)\n",
    "\n",
    "#img_loader = DataLoader(dataset, batch_size=64, shuffle=False) # 9min 2s\n",
    "#img_loader = DataLoader(dataset, batch_size=1, shuffle=False) # 12min43s w/o transforms\n",
    "#img_loader = DataLoader(dataset, batch_size=2, shuffle=False)\n",
    "#img_loader = DataLoader(dataset, batch_size=4, shuffle=False) # 8m w transforms (necessary for elevated batch size)\n",
    "#img_loader = DataLoader(dataset, batch_size=8, shuffle=False) # 8min 28s\n",
    "img_loader = DataLoader(dataset, batch_size=16, shuffle=False) # 8min 32s\n",
    "# trying again with larger batch after adding transforms\n",
    "\n",
    "for i, _ in enumerate(img_loader):\n",
    "    continue\n",
    "# yiiikes... 12min43s just to iterate through the files and load them into tensors... yeesh.\n",
    "# seriously doubt I'm multiprocessing here. Possibly low batch size kills multiproc as option?\n",
    "\n",
    "\n",
    "# TypeError: default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; \n",
    "# found <class 'PIL.JpegImagePlugin.JpegImageFile'>\n",
    "#\n",
    "# fixed above error with torchvision.io.read_image\n",
    "# Now we've got an uncollated shape issue\n",
    "#   RuntimeError: stack expects each tensor to be equal size, but got [3, 3024, 4032] at entry 0 and [3, 2268, 4032] at entry 16\n",
    "# could add a collate fn per: https://discuss.pytorch.org/t/dataloader-gives-stack-expects-each-tensor-to-be-equal-size-due-to-different-image-has-different-objects-number/91941\n",
    "# ... but then the padding would probably fuck up the CLIP featurization\n",
    "# could go the other way with a crop, but who even knows what I'd be cropping. How even does the model address the input resolution?\n",
    "# whatever, let's see how long it takes with a batch_size of 1\n",
    "#\n",
    "# RuntimeError: Unsupported marker type 0xa3\n",
    "# 'C:\\\\Users\\\\shagg\\\\Pictures\\\\phone backup\\\\20180913_145729.jpg' - panoramic, 35mb. \n",
    "#\n",
    "# crushing error by returning tensor of zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(46,\n",
       " ('C:\\\\Users\\\\shagg\\\\Pictures\\\\phone backup\\\\20180913_144244.jpg',\n",
       "  'C:\\\\Users\\\\shagg\\\\Pictures\\\\phone backup\\\\20180913_144258.jpg',\n",
       "  'C:\\\\Users\\\\shagg\\\\Pictures\\\\phone backup\\\\20180913_144303.jpg',\n",
       "  'C:\\\\Users\\\\shagg\\\\Pictures\\\\phone backup\\\\20180913_144648.jpg'))"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# OSError: broken data stream when reading image file\n",
    "i, _[1] # the fuck? Why is path 4 files? oh right, batched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(94,\n",
       " ('C:\\\\Users\\\\shagg\\\\Pictures\\\\phone backup\\\\20180913_145647.jpg',\n",
       "  'C:\\\\Users\\\\shagg\\\\Pictures\\\\phone backup\\\\20180913_145648.jpg'))"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# OSError: broken data stream when reading image file\n",
    "i, _[1] # \n",
    "# looks like the issue is with 'C:\\\\Users\\\\shagg\\\\Pictures\\\\phone backup\\\\20180913_145648.jpg' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.5255, 0.5216, 0.5255,  ..., 0.1961, 0.2078, 0.1333],\n",
       "         [0.5176, 0.5176, 0.5255,  ..., 0.2078, 0.1961, 0.1333],\n",
       "         [0.5216, 0.5255, 0.5333,  ..., 0.1686, 0.1529, 0.1294],\n",
       "         ...,\n",
       "         [0.9804, 0.9765, 0.9765,  ..., 0.0275, 0.0392, 0.0392],\n",
       "         [0.9765, 0.9765, 0.9765,  ..., 0.0235, 0.0353, 0.0314],\n",
       "         [0.9765, 0.9725, 0.9765,  ..., 0.0157, 0.0314, 0.0353]],\n",
       "\n",
       "        [[0.6196, 0.6157, 0.6118,  ..., 0.1569, 0.1686, 0.0941],\n",
       "         [0.6118, 0.6118, 0.6118,  ..., 0.1686, 0.1569, 0.0941],\n",
       "         [0.6078, 0.6118, 0.6196,  ..., 0.1294, 0.1137, 0.0902],\n",
       "         ...,\n",
       "         [0.9765, 0.9725, 0.9725,  ..., 0.0235, 0.0353, 0.0353],\n",
       "         [0.9725, 0.9725, 0.9725,  ..., 0.0196, 0.0314, 0.0275],\n",
       "         [0.9725, 0.9686, 0.9725,  ..., 0.0118, 0.0275, 0.0314]],\n",
       "\n",
       "        [[0.7608, 0.7569, 0.7569,  ..., 0.1608, 0.1725, 0.0980],\n",
       "         [0.7529, 0.7529, 0.7569,  ..., 0.1725, 0.1608, 0.0980],\n",
       "         [0.7529, 0.7569, 0.7647,  ..., 0.1333, 0.1176, 0.0941],\n",
       "         ...,\n",
       "         [0.9608, 0.9569, 0.9569,  ..., 0.0078, 0.0196, 0.0196],\n",
       "         [0.9569, 0.9569, 0.9569,  ..., 0.0039, 0.0157, 0.0118],\n",
       "         [0.9569, 0.9529, 0.9569,  ..., 0.0000, 0.0118, 0.0157]]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "im_test = Image.open('C:\\\\Users\\\\shagg\\\\Pictures\\\\phone backup\\\\20180913_145729.jpg')\n",
    "#im_test # PIL at least can read the last frame of the panorama.\n",
    "ToTensor()(im_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.5255, 0.5216, 0.5255,  ..., 0.1961, 0.2078, 0.1333],\n",
       "         [0.5176, 0.5176, 0.5255,  ..., 0.2078, 0.1961, 0.1333],\n",
       "         [0.5216, 0.5255, 0.5333,  ..., 0.1686, 0.1529, 0.1294],\n",
       "         ...,\n",
       "         [0.9804, 0.9765, 0.9765,  ..., 0.0275, 0.0392, 0.0392],\n",
       "         [0.9765, 0.9765, 0.9765,  ..., 0.0235, 0.0353, 0.0314],\n",
       "         [0.9765, 0.9725, 0.9765,  ..., 0.0157, 0.0314, 0.0353]],\n",
       "\n",
       "        [[0.6196, 0.6157, 0.6118,  ..., 0.1569, 0.1686, 0.0941],\n",
       "         [0.6118, 0.6118, 0.6118,  ..., 0.1686, 0.1569, 0.0941],\n",
       "         [0.6078, 0.6118, 0.6196,  ..., 0.1294, 0.1137, 0.0902],\n",
       "         ...,\n",
       "         [0.9765, 0.9725, 0.9725,  ..., 0.0235, 0.0353, 0.0353],\n",
       "         [0.9725, 0.9725, 0.9725,  ..., 0.0196, 0.0314, 0.0275],\n",
       "         [0.9725, 0.9686, 0.9725,  ..., 0.0118, 0.0275, 0.0314]],\n",
       "\n",
       "        [[0.7608, 0.7569, 0.7569,  ..., 0.1608, 0.1725, 0.0980],\n",
       "         [0.7529, 0.7529, 0.7569,  ..., 0.1725, 0.1608, 0.0980],\n",
       "         [0.7529, 0.7569, 0.7647,  ..., 0.1333, 0.1176, 0.0941],\n",
       "         ...,\n",
       "         [0.9608, 0.9569, 0.9569,  ..., 0.0078, 0.0196, 0.0196],\n",
       "         [0.9569, 0.9569, 0.9569,  ..., 0.0039, 0.0157, 0.0118],\n",
       "         [0.9569, 0.9529, 0.9569,  ..., 0.0000, 0.0118, 0.0157]]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ToTensor()(im_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
